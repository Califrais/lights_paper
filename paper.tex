\documentclass[11pt]{article}

\usepackage[colorlinks=true,
            linkcolor=red,
            urlcolor=blue,
            citecolor=blue,
            breaklinks=true]{hyperref}
\usepackage{breakurl} 
\usepackage[a4paper, left=3cm, right=3cm, top=3cm, bottom=3cm]{geometry}
\RequirePackage{amsmath,amsfonts, amssymb,amsthm}

\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{mathrsfs} 
\usepackage{natbib}
\usepackage{setspace}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}

\hypersetup{pdfauthor = {Simon Bussy}}

\usepackage{authblk}
\title{\vspace{-.5cm} Lights: a generalized joint model for high-dimensional multivariate longitudinal data and censored durations \vspace{.5cm}}
\author[1]{Simon Bussy\thanks{Corresponding author: \href{mailto:simon.bussy@gmail.com}{\texttt{simon.bussy@gmail.com}}}}
\author[2]{Antoine Barbieri}
\author[1]{Sarah Zohar}
\author[1,3]{Anne-Sophie Jannot}
\affil[1]{INSERM, UMRS 1138, Centre de Recherche des Cordeliers, Paris, France}
\affil[2]{INSERM, UMR 1219, Bordeaux Population Health Research Center, Univ. Bordeaux, France}
\affil[3]{Biomedical Informatics and Public Health Department, EGPH, APHP, Paris, France}
\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}

\DeclareMathOperator{\argmin}{argmin}

\newcommand{\dd}{\mathrm{d}}
\newcommand{\ind}[1]{\mathds{1}_{#1}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\cM}{\mathcal M}
\newcommand{\cD}{\mathcal D}
\newcommand{\cC}{\mathcal C}
\newcommand{\cN}{\mathcal N}
\newcommand{\cA}{\mathcal A}
\newcommand{\R}{\mathds R}
\newcommand{\N}{\mathds N}
\newcommand{\E}{\mathds E}
\renewcommand{\P}{\mathds P}
\newcommand{\bSigma}{\textbf{$\Sigma$}}

\date{}

\begin{document}

\maketitle

\vspace{-.5cm}

\begin{abstract}

This paper introduces a prognostic method called \textit{lights} to deal with the problem of joint modeling of longitudinal data and censored durations, where a large number of longitudinal features are available. Yet there is no standard model so far to learn from such high-dimensional multivariate longitudinal data in a survival analysis setting.
The statistical performance of the method is examined on an extensive Monte Carlo simulation study, and finally illustrated on a publicly available dataset.
On these high-dimensional datasets, our proposed method significantly outperforms the state-of-the-art survival models regarding risk prediction in terms of C-index, with a computing time orders of magnitude faster. In addition, it provides powerful interpretability by automatically pinpointing significant features being relevant from a clinical perspective. Thus, we propose a powerfull tool for personalized medicine, with the ability of automatically determining significant prognostic longitudinal biomarkers, which is of increasing importance in many areas of medicine.
\\

\noindent
\emph{Keywords.} High-dimensional estimation; Joint modeling; Multivariate longitudinal data; Survival analysis
\end{abstract}

\section{Introduction}

In many clinical studies, it has become increasingly common to record the values of longitudinal features (e.g. biomarkers) until the occurrence of an event of interest for a subject. The ``joint modeling'' approaches, namely modeling the longitudinal and survival outcomes through a joint likelihood model rather than separately, has received considerable attention during the past two decades~\citep{wulfsohn1997joint}. Numerical studies suggest that these approaches are among the most satisfactory to combine information~\citep{tsiatis2004joint,yu2004joint}. They have the advantage of making more efficient use of the data since information about survival also goes into modeling the longitudinal features. In addition, they produce unbiased estimates and do not rely on approximations for incorporating conplex longitudinal trajectories.

\paragraph{The high-dimensional longitudinal data context.}
With the development of electronic health records, high-dimensional settings are becoming increasingly frequent in various contexts where the number of available features to consider as potential risk factors is tremendous.
Moreover, with an increased focus on personalised medicine, the need to implement multivariate models that account for a large number of longitudinal outcomes is critical. Despite this, joint models have predominantly focused on univariate data, with attempts to fit multiple univariate joint models separately~\citep{wang2012joint}, which is inefficient~\citep{lin2002maximum}. 
Despite many multivariate models being presented in full generality, questions arrising from the high-dimensional context -- computational power, limits in numerical estimation, or sample size for instance -- are never considered in analyses (to the best of our knowledge), and the number of longitudinal outcomes considered in numerical studies are often very low (see~\citet{hickey2016joint} for a complete review). For instance,~\citet{jaffa2014joint} only considers 3 longitudinal outcomes in the simulation study while mentioning a ``high-dimensional multivariate longitudinal data'' context.


\paragraph{General framework.}

The setting of this paper is such that we want to incorporate high-dimensional time-dependent (longitudinal) features measured with error in a survival model. Let us consider the usual survival analysis framework.
Following~\citet{AndBorGilKei-93}, let non-negative random variables $T^\star$ and $C$ stand for the times of the event of interest and censoring times respectively. The event of interest could be for instance survival time, re-hospitalization, relapse or disease progression.
We then denote $T$ the right-censored time and $\Delta$ the censoring indicator, defined as 
\begin{equation*}
T = T^\star \wedge C \quad \text{and} \quad \Delta = \ind{}({\{T^\star \leq C\}})
\end{equation*}
respectively, where $a \wedge b$ denotes the minimum between two numbers $a$ and $b$, and $\ind{}(\cdot)$ the indicator function taking the value $1$ if the condition in $(\cdot)$ is satisfied and $0$ otherwise.

Let $X$ denotes the $p$-dimensional vector of time-independent features (e.g. patients characteristics, therapeutic strategy, or omics features recorded at the begining of a study), and let  $Y(t) = \big(Y^1(t), \ldots, Y^L(t) \big)^\top \in \R^L$ denote the value of the $L$-dimensional longitudinal outcome at time point $t \geq 0$, with $L > 0$.


\paragraph{Heterogeneity of the population.}

An assumption of heterogeneity within the patient population is frequently relevant in medical research where several differing profiles of subjects are expected~\citep{bussy2019c}. To take account of this, we introduce a latent variable $G \in \{0, \ldots, K-1\}$ modeling the $K \geq 1$ subgroups of different risk. Let us denote
\[ \pi_{\xi_k}(x) = \P[G=k|X=x] \]
the latent class membership probability given time-independent features $x \in \R^p$, and consider a softmax link function given by
\[ \pi_{\xi_k}(x) = \dfrac{e^{x^\top\xi_k}}{\sum_{k=0}^{K-1}e^{x^\top\xi_k}} \]
where $\xi_k \in \R^p$ denotes a vector of coefficients that quantifies the impact of each time-independent features on the probability that a subject belongs to the $k$-th group.  From now on, all computations are done conditionally on features $x$.

Then, the idea is to allow flexibility in modeling the dependency between the longitudinal features and the event time.
Joint models have the ability to produce prognostic predictions that can be dynamically updated according to the observed trajectory of the features, thus offering a powerful tool for clinical decision making in patient monitoring. But those questions are never considered in a high-dimensional framework~\citep{proust2009development,rizopoulos2011dynamic}.

When validating predictions or comparing performances of competing models in a survival context, one can be interested either in the discriminative power of the predictive rule and use concordance measures such as the C-index~\citep{heagerty2005survival}, or in the predictive accuracy of the rule~\citep{schemper2000predictive}.
Developments in the field of joint modeling have primarily focused on modeling and estimation, and most studies do not consider goodness of fit nor generalisation power [REF!]. However, practitioners will naturally require predictive prognostic tools to evaluate model fits and compare models. 

Once the model is train, posterior risk classification can be made through posterior estimates of the latent class membership probabilities, and latent class model may be particularly suited for prediction problems. We propose to use this probabilities as discriminative marker rule for risk prediction and use it in the cross-validation procedure for selecting the best regularization hyper-parameters. Indeed, it makes sense from a practical point of view, since one want to obtain prognostic predictions as well as interpretations of the high-dimensional longitudinal features.


\paragraph{Main contribution.} 

In this paper, we propose a method called \textit{lights} (generaLized joInt hiGH-dimensional longiTudinal Survival) that \texttt{blabla... ajout description courte des caractéristiques du modèle une fois les maths terminées}.


\paragraph{Organization of the paper.}

A precise description of the model is given in Section~\ref{sec:Method}.
Section~\ref{sec:inference} focuses on a regularized version of the
model to exploit dimension reduction and prevent overfitting. Inference is
presented under this framework, as well as the convergence properties of the developed algorithm.
Section~\ref{sec:Performance evaluation} presents the simulation procedure used to evaluate the performance of our method and compares it with state-of-the-art ones. In Section~\ref{sec:application}, we apply our method to high-dimensional publicly available datasets. Finally, we discuss the obtained results in Section~\ref{sec:conclusion}.

\paragraph{Notations.}

Throughout the paper, for every $q > 0,$ we denote by $\norm{v}_q$ the usual $\ell_q$-quasi norm of a vector $v \in \R^m,$ namely
 $\norm{v}_q =(\sum_{k=1}^m|v_k|^q)^{1/q}$. We also denote $\norm{v}_0 = |\{k : v_k \neq 0\}|$, where $|A|$ stands for the cardinality of a finite set $A$.
 For $u, v \in \R^m$, we denote by $u \odot v$ the Hadamard product $u\odot v =(u_1v_1, \ldots, u_mv_m)^\top.$ 
For a squared matrix $M$, $\text{vech}(M)$ stacks columns of $M$ one under another in a single vector, starting each column at its diagonal element.
Finally, we write, for short, $\mathbf{1}_m$  (resp. $\mathbf{0}_m$) for the vector of $\R^m$ having all coordinates equal to one (resp. zero).


\section{Method}
\label{sec:Method}

In this section, we descibe the longitudinal and time-to-event submodels, as well as the required hypothesis in order to write a likelihood and draw inference for the lights model.

\paragraph{Group-specific marker trajectories.}

We suppose a group-specific marker trajectory and a generalized linear mixed model for each longitudinal marker given subgroup $G$, so that for the $l$-th outcome at time $t \geq 0$ one has
\begin{equation}
	\label{eq:link-function}
 	h_l\big(\E[Y^l(t)|b^l, G=k]\big) = m_k^l(t)
 \end{equation}
where $h_l$ denotes a known one-to-one link function, and $m_k^l$ the linear predictor such that
\[ m_k^l(t) = u^l(t)^\top\beta_k^l + v^l(t)^\top b^l \]
where $u^l(t) \in \R^{p_l}$ is a row vector of (possibly) time-varying features with corresponding unknown fixed effect parameters $\beta_k^l$, and $v^l(t) \in \R^{r_l}$ is a row vector of (possibly) time-varying features with $r_l \leq p_l$ and where the corresponding subject-and-longitudinal outcome specific random effects $b^l$.
A suitable distributional assumption for the random effects component is a zero-mean multivariate normal distribution~\citep{hickey2016joint}, that is
\[ b^l \sim \cN(0, D_{ll}) \]
with $D_{ll} \in \R^{r_l \times r_l}$ the unstructured variance-covariance matrix that does not depend on the group membership. 
To account for dependence between the different longitudinal outcome types, we let $\text{Cov}[b^l,b^{l'}] = D_{ll'}$ for $l \ne l'$ and we denote
\[ D = 
\begin{bmatrix}
  D_{11} & \cdots & D_{1L}\\
  \vdots &  \ddots & \vdots \\
  D_{1L}^\top & \cdots & D_{LL}
\end{bmatrix}
\]
the global variance-covariance matrix.
In the sequel of the paper, our aim is to associate the true and unobserved value $m_k^l(t)$ of the $l$-th longitudinal outcome at time $t$ with the event outcome $T^\star$.

\paragraph{Group-specific risk of event.}

To quantify the effect of the longitudinal outcomes on the risk for an event, we use a relative risk model of the form
\begin{equation}
	\label{eq:intensity-model}
	\lambda(t|\cM_k(t), G = k, w) = \lambda_0(t) \exp \Big\{ w^\top \gamma_{k,0} + \sum_{l=1}^L \sum_{a=1}^\cA \gamma_{k,a}^l \varphi_a(t, \beta_k^l, b^l) \Big\},
\end{equation}
where $\lambda_0(\cdot)$ denotes an unspecified baseline hazard function and given that $G = k$, $\cM_k(t) = \{m_k^l(u), 0 \leq u < t\}$ denotes the history of the true unobserved longitudinal process up to time $t$, $w$ is a $q$-vector of time-independant features with corresponding fixed effects $\gamma_k \in \R^q$, and $\gamma_{k,a}^l \in \R$ is the joint model association parameter corresponding to a known functional $\varphi_a$ defining a shared association. Details on the choice of $(\varphi_a)_{a \in \cA}$ are given in Section~\ref{sec:numerical details}. From now on, all computations are done conditionally on features $w$.
 
\paragraph{Likelihood.}

Consider an independent and identically distributed (i.i.d.) cohort of $n$ subjects
\[ \cD = \big\{ (x_1, y_1^1, \ldots, y_1^L, t_1, \delta_1), \ldots, (x_n, y_n^1, \ldots, y_n^L, t_n, \delta_n) \big\} \]
where for each subject $i=1, \ldots, n$, process $Y_i^l$ is measured $n_i^l$ times at $t_{i1}^l, \ldots, t_{in_i^l}^l$ (which can differ between subjects and outcomes) with $t_{ij}^l \leq t_{ij+1}^l$ for all $j=1, \ldots, n_i^l-1$ and such that
\[ y_i^l=(y_{i1}^l, \ldots, y_{in_i^l}^l)^\top \in \R^{n_i^l} \quad \text{ with } \quad y_{ij}^l=Y_i^l(t_{ij}^l) \]
for all $l=1, \ldots, L$. 
For the $i$-th subject, let us denote 
\[
\left\{
    \begin{array}{ll}
        y_i &= ({y_i^1}^\top, \ldots, {y_i^L}^\top)^\top \in \R^{n_i},\\
        b_i &= ({b_i^1}^\top, \ldots, {b_i^L}^\top)^\top \in \R^r,
    \end{array}
\right.
\]
with $n_i = \sum_{l=1}^L n_i^l$ and $r = \sum_{l=1}^L r_l$ the total number of longitudinal measurements (for subject $i$) and the total dimension of the random effects respectively, as well as the following design matrices
\[ U_i = 
\begin{bmatrix}
  U_{i1} & \cdots & 0\\
  \vdots &  \ddots & \vdots \\
  0 & \cdots & U_{iL}
\end{bmatrix} 
\in \R^{n_i \times p}
\qquad \text{and} \qquad
V_i = 
\begin{bmatrix}
  V_{i1} & \cdots & 0\\
  \vdots &  \ddots & \vdots \\
  0 & \cdots & V_{iL}
\end{bmatrix}
\in \R^{n_i \times r}
\]
where for all $l=1, \ldots, L$, one writes
\[
\left\{
    \begin{array}{ll}
        U_{il} &= [u_i^l(t_{i1}^l) \cdots u_i^l(t_{in_i^l}^l)]^\top \in \R^{n_i^l \times p_l},\\
        V_{il} &= [v_i^l(t_{i1}^l) \cdots v_i^l(t_{in_i^l}^l)]^\top \in \R^{n_i^l \times r_l}.
    \end{array}
\right.
\]
Note that from now on, all computations are done conditionally on the design matrices $(U_i)_{i=1, \dots, n}$ and $(V_i)_{i=1, \dots, n}$.
Suppose that conditional on the random effects, each $Y_i^l$ is independent and the censoring times $T_i$ are independent of $b_i$; both are standard modelling assumptions.
For all $k = 0, \ldots, K-1$, we denote 
\[ \beta_k = ({\beta_k^1}^\top, \ldots, {\beta_k^L}^\top) \in \R^p \] 
with $p = \sum_{l=1}^L p_l$ and 
\[M_{ik} = U_i\beta_k + V_ib_i \in \R^{n_i}. \]

Given~\eqref{eq:link-function}, each $y_i^l$ is assumed to be from a one-parameter exponential family with respect to a reference measure which is either the Lebesgue measure (e.g. in the Gaussian case) or the counting measure (e.g. in the logistic cases).
The conditional distribution of $y_i|b_i, G_i=k$ is then assumed to be from a distribution with a density of the form
\begin{equation}
	\label{eq:exp-family-density}
	f(y_i|b_i, G_i=k) = \exp \big\{(y_i \odot \Phi_i)^\top M_{ik} - c_\phi(M_{ik}) + d_\phi(y_i) \big\},
\end{equation}
with 
\[ \Phi_i = [\phi_1^{-1} {\textbf{1}_{n_i^1}}^\top \cdots \phi_L^{-1} {\textbf{1}_{n_i^L}}^\top]^\top \in \R^{n_i} \] 
and $\phi = (\phi_1, \ldots, \phi_L)^\top \in \R^L$.
The density described in~\eqref{eq:exp-family-density} encompasses several distributions, see Table~\ref{table:glm}. The functions $c_\phi(\cdot)$ and $d_\phi(\cdot)$ are known as well as the dispersion parameters $\phi_l$, while parameters $\beta_k$ have to be estimated. Note that $d_\phi(\cdot)$ is related to the normalizing constant.
\begin{table}[htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccccc}
\toprule
 Model & Support & Use cases & $\phi_l$ & $h_l(\cdot)$ in~\eqref{eq:link-function} & $c_{\phi_l}(\cdot)$ \\
 \midrule
 Normal & $\R$ & Continuous response data & $\sigma_l^2$ & $z \mapsto z$ & $z \mapsto \phi_l^{-1}\frac{z^2}{2}$ \\ 
 Categorical & $\{0, 1\}^C$ & Outcome with $C$ modalities & 1 & $z \mapsto \log\Big(\dfrac{z}{1 - z} \Big)$ & $z \mapsto \phi_l^{-1} \frac{e^z}{1+e^z}$ \\
 Poisson & $\N$ & Count of occurrences & 1 &  $z \mapsto e^z$ & $z \mapsto \phi_l^{-1} e^z$ \\
 \bottomrule
\end{tabular}}
\caption[table]{\small Examples of standard distributions that fit in the considered setting.}
\label{table:glm}
\end{table}

Let us denote 
\[ \theta = (\xi_0, \ldots, \xi_{K-1}, \beta_0, \ldots, \beta_{K-1}, \phi^\top, \text{vech}(D), \lambda_0(t), \gamma_0, \ldots, \gamma_{K-1})^\top \]
the collection of unknown parameters that we want to estimate, with $\gamma_k=$ \texttt{to be defined properly}.
To write the log-likelihood $\ell_n(\theta)$ (rescaled by $n^{-1}$)  for samples in $\cD$, corresponding to the joint distribution of the time-to-event and longitudinal outcomes, let us assume that the random effects vector $b_i$ and the group membership account for the association between the longitudinal and event outcomes, that is
\begin{equation}
	\label{eq:ind-hyp}
	f(t_i, \delta_i, y_i | b_i, G_i = k ; \theta) = f(t_i, \delta_i| b_i, G_i = k ; \theta) f(y_i | b_i, G_i = k ; \theta)
\end{equation}
for all $i=1, \ldots, n$. Hypothesis~\eqref{eq:ind-hyp} is \texttt{faire le lien avec les hyps classiques de la litterature} [REFs].
Then, one has
\begin{align}
	\label{eq:log-lik}
	\ell_n(\theta) &= \ell_n(\theta ; \cD) \nonumber \\
	&= n^{-1} \sum_{i=1}^n \log \int_{-\infty}^{+\infty} \sum_{k=0}^{K-1} \pi_{\xi_k}(x_i) f(t_i, \delta_i| b_i, G_i = k ; \theta) f(y_i | b_i, G_i = k ; \theta) f(b_i ; \theta) \dd b_i,
\end{align}
where 
\[f(t_i, \delta_i| b_i, G_i = k ; \theta) = \big[\lambda(t_i|\cM_k(t_i), G_i = k, w_i)\big]^{\delta_i} \exp \Big\{-\int_0^{t_i} \lambda(u|\cM_k(u), G_i = k, w_i) \dd u \Big\} \]
and
\[f(b_i ; \theta) = (2 \pi)^{-\dfrac r2} |D|^{-\dfrac12} \exp \Big\{ -\dfrac12 b_i^\top D^{-1}b_i \Big\}.\]

\section{Inference}
\label{sec:inference}

In order to avoid overfitting and to improve the prediction power of the lights model, we use Elastic-Net regularization~\citep{zou2005regularization} by minimizing the penalized objective
\begin{equation}
	\ell_n^\text{pen}(\theta) = - \ell_n(\theta) + \sum_{k=0}^{K-1} \zeta_{1,k} \norm{\xi_k}_{\text{E-N}, \eta} + \zeta_{2,k} \norm{\beta_k}_{\text{E-N}, \eta} + \zeta_{3,k} \norm{\gamma_k}_{\text{E-N}, \eta}
\end{equation}
where for all $k=0, \ldots, K-1$, we add a linear combination of the lasso ($\ell_1$) and ridge (squared $\ell_2$) penalties of the vectors $\xi_k$, $\beta_k$ and $\gamma_k$ for tuning hyperparameters $(\zeta_{1,k}, \zeta_{2,k}, \zeta_{3,k})^\top \in \R_+^3$, with a fixed $\eta \in [0, 1]$ and 
\[ \norm{v}_{\text{E-N}, \eta} = (1-\eta)\norm{v}_1 + \dfrac\eta2 \norm{v}_2^2 \] 
for any vector $v$. \texttt{expliquer interpretation de $(\zeta_{1,k}, \zeta_{2,k}, \zeta_{3,k})$}

One advantage of this regularization method is its ability to perform model selection (the lasso part) and pinpoint the most important
features relatively to the prediction objective. On the other hand, the ridge part allows to handle potential correlation between time-independent features or between longitudinal outcomes. Note that in practice, the intercept is not regularized.

\subsection{A Fast Stochastic Approximation of a Quasi-Newton EM}
\label{sec:FSA-QNEM}

\texttt{dérouler l'algo. idée: QNEM + \citet{karimi2020f}}

\subsection{Convergence to a stationary point}
\label{sec:convergence}

\texttt{théorème avec étude de convergence}

\subsection{Cross-validation procedure}
\label{sec:CV-procedure}

\paragraph{Posterior group membership probabilities.}

Les us denote
\begin{equation}
	\hat \pi_{ik}^{Y,T} = \P_{\hat \theta}[G_i = k | y_i, t_i, \delta_i]
\end{equation}

\texttt{utiliser ce marker pour cross-val les $(\zeta_{1,k}, \zeta_{2,k}, \zeta_{3,k})$ avec du C-index + use randomized grid search~\citep{bergstra2012random}}


\section{Performance evaluation}
\label{sec:Performance evaluation}

In this section, we first give numerical details and briefly introduce the models we consider for performance comparisons. Then, we provide details regarding the simulation study and data generation. The chosen metrics for evaluating performances are then presented, followed by the results.

\subsection{Numerical details}
\label{sec:numerical details}

\paragraph{Specification of the design matrices.}
 
In many practical applications, subjects show highly nonlinear longitudinal trajectories. In~\eqref{eq:log-lik}, the complete longitudinal history is required for the computation of the survival function. Hence, in order to produce a good estimate of $\cM(t)$, we consider a flexible representations for $u^l(t)$ and $v^l(t)$ using a high-dimensional vector of functions of time $t$ expressed in terms of splines.

\paragraph{Specification of the functionals $(\varphi_a)_{a\in \cA}$.}


\subsection{Competing models}
\label{sec:competing models}

State-of-the-art implementations in \texttt{R}: \\
\\
- recent package in multivariate settings: \texttt{joineRML}
\citep{hickey2018joinerml} \\
- reference one: \texttt{JM}
\citep{rizopoulos2010jm} \\
- MCMC methods: \texttt{JMbayes} 
\citep{rizopoulos2014r} (voir slide 132 \url{http://www.drizopoulos.com/courses/Int/JMwithR_CEN-ISBS_2017.pdf})\\ 
- \texttt{joineR}
\citep{philipson2018package} \\
- classical time-dependent Cox model: \texttt{survival}, explanations on how to do this in \citet{zhang2018time} \\
\\
> still nothing in \texttt{Python} ! I'll propose an implementation of the model in \texttt{Python} (today paramount in machine learning, it will be an additional asset of the paper).

\subsection{Simulation design}
\label{simulation design}

In order to assess the proposed method, we perform an extensive Monte Carlo simulation study. \\
\\
simulation of time-varying features in a Cox model: see \citet{therneau2017using}


\subsection{Metric}
\label{sec:Metrics}

We detail in this section the metric considered to evaluate risk prediction performances. Let us denote by $M$ the marker under study. We denote by $h$ the probability density function of marker $M$, and assume that the marker is measured once at $t = 0$.

A common concordance measure that does not depend on time is the C-index~\citep{harrell1996tutorial} defined by
\begin{equation*}
  \cC =\P[M_i > M_j | T^\star_i < T^\star_j],
\end{equation*}
with $i \neq j$ two independent subjects (which does not depend on $i, j$ under the $i.i.d.$ sample hypothesis). 
In our case,  $T^\star$ is subject to right censoring, so one would typically consider the modified $\cC_\tau$ defined by
\begin{equation*}
  \cC_\tau =\P[M_i > M_j | T_i < T_j , T_i < \tau],
\end{equation*}
with $\tau$ corresponding to the fixed and prespecified follow-up period duration \citep{heagerty2005survival}. A Kaplan-Meier estimator for the censoring distribution leads to a nonparametric and consistent estimator of $\cC_\tau$ \citep{uno2011c}, already implemented in the \texttt{R} package \texttt{survival}.
Hence in the following, we consider the C-index metric to assess performances.


\subsection{Results of simulation}
\label{Results}
Let us present now the simulation results.


\section{Applications}
\label{sec:application}

\texttt{PBC dataset: open source, multidim mais dimension $\sim$15 (voir \url{https://rdrr.io/cran/JM/man/pbc.html}). \\ 
essayer de trouver 1 dataset en + grande dimension?}

\

\texttt{ other idea ++ use MIMIC II or III dataset ! \\
\url{https://www.nature.com/articles/sdata201635/}\\
\url{https://ieeexplore.ieee.org/abstract/document/1166854}\\
\url{https://www.sciencedirect.com/science/article/abs/pii/S2213260014702395}\\
\url{https://www.sciencedirect.com/science/article/pii/S2352556818302169}\\
}


\section{Conclusion}
\label{sec:conclusion}


\section*{Software}
All the methodology discussed in this paper is implemented in \texttt{Python} and \texttt{R}. The code is available from \href{https://github.com/SimonBussy/?}%
{https://github.com/SimonBussy/?} in the form of annotated programs, together with a notebook tutorial.

\section*{Acknowledgements}
\textit{Conflict of Interest}: None declared.

\appendix

\begin{center}
\LARGE \textbf{Appendices}
\end{center}

\section{Proofs}
\label{sec:proofs}


\bibliography{biblio}
\bibliographystyle{plainnat}{}
\end{document}
