\documentclass[11pt]{article}

\usepackage[colorlinks=true,
            linkcolor=red,
            urlcolor=blue,
            citecolor=blue,
            breaklinks=true]{hyperref}
\usepackage{breakurl} 
\usepackage[a4paper, left=3cm, right=3cm, top=3cm, bottom=3cm]{geometry}
\RequirePackage{amsmath,amsfonts, amssymb,amsthm}

\hypersetup{pdfauthor = {Simon Bussy}}

\usepackage{authblk}
\title{\vspace{-.5cm} Lights: a generalized joint model for high-dimensional multivariate longitudinal data and censored durations \vspace{.5cm}}
\author[1]{Simon Bussy\thanks{Corresponding author: \href{mailto:simon.bussy@gmail.com}{\texttt{simon.bussy@gmail.com}}}}
\author[2]{Antoine Barbieri}
\author[1]{Sarah Zohar}
\author[1,3]{Anne-Sophie Jannot}
\affil[1]{INSERM, UMRS 1138, Centre de Recherche des Cordeliers, Paris, France}
\affil[2]{INSERM, UMR 1219, Bordeaux Population Health Research Center, Univ. Bordeaux, France}
\affil[3]{Biomedical Informatics and Public Health Department, EGPH, APHP, Paris, France}
\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}

\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{mathrsfs} 
\usepackage{natbib}
\usepackage{setspace}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{booktabs}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{orange}{#1}}
\SetCommentSty{mycommfont}
\SetKwInput{KwInput}{Input} 
\SetKwInput{KwOutput}{Output}

\newtheorem{assumption}{Assumption}{\bf}{\rm} 

\DeclareMathOperator{\argmin}{argmin}

\newcommand{\dd}{\mathrm{d}}
\newcommand{\ind}[1]{\mathds{1}_{#1}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\cM}{\mathcal M}
\newcommand{\cD}{\mathcal D}
\newcommand{\cC}{\mathcal C}
\newcommand{\cN}{\mathcal N}
\newcommand{\cA}{\mathcal A}
\newcommand{\cQ}{\mathcal Q}
\newcommand{\R}{\mathds R}
\newcommand{\N}{\mathds N}
\newcommand{\E}{\mathds E}
\renewcommand{\P}{\mathds P}
\newcommand{\bSigma}{\textbf{$\Sigma$}}

\date{}

\begin{document}

\maketitle

\vspace{-.5cm}

\begin{abstract}

This paper introduces a prognostic method called \textit{lights} to deal with the problem of joint modeling of longitudinal data and censored durations, where a large number of longitudinal features are available. Yet there is no standard model so far to learn from such high-dimensional multivariate longitudinal data in a survival analysis setting.
Features are extracted from the longitudinal processes and included as potential risk factor in a group-specific Cox model with high-dimensional shared associations. Appropriate penalties are then used during inference to allow flexibility in modeling the dependency between the longitudinal features and the event time.
The statistical performance of the method is examined on an extensive Monte Carlo simulation study, and finally illustrated on publicly available datasets.
On these high-dimensional datasets, our proposed method significantly outperforms the state-of-the-art survival models regarding risk prediction in terms of C-index, with a computing time orders of magnitude faster. In addition, it provides powerful interpretability by automatically pinpointing significant features being relevant from a clinical perspective. Thus, we propose a powerfull tool for personalized medicine, with the ability of automatically determining significant prognostic longitudinal biomarkers, which is of increasing importance in many areas of medicine.
\\

\noindent
\emph{Keywords.} High-dimensional estimation; Joint modeling; Multivariate longitudinal data; Survival analysis
\end{abstract}

\section{Introduction}

In many clinical studies, it has become increasingly common to record the values of longitudinal features (e.g. biomarkers) until the occurrence of an event of interest for a subject. The ``joint modeling'' approaches, namely modeling the longitudinal and survival outcomes through a joint likelihood model rather than separately, has received considerable attention during the past two decades~\citep{tsiatis2004joint}. Numerical studies suggest that these approaches are among the most satisfactory to combine information~\citep{yu2004joint}. They have the advantage of making more efficient use of the data since information about survival also goes into modeling the longitudinal features. In addition, they produce unbiased estimates and do not rely on approximations for incorporating conplex longitudinal trajectories. Most developments have either focused on shared random-effect models (SREMs)~\citep{wulfsohn1997joint}, in which characteristics of the longitudinal processes (for instance functions of the random effects) are included as features in the survival model ; or on joint latent class models (JLCMs)~\citep{vermunt2003latent}, in which the population is considered as heterogeneous, with the assumption that there exist homogeneous latent subgroups sharing the same marker trajectories and the same prognostic.

\paragraph{The high-dimensional longitudinal data context.}
With the development of electronic health records, high-dimensional settings are becoming increasingly frequent in various contexts where the number of available features to consider as potential risk factors is tremendous.
Moreover, with an increased focus on personalised medicine, the need to implement multivariate models that account for a large number of longitudinal outcomes is critical. Despite this, joint models have predominantly focused on univariate data, with attempts to fit multiple univariate joint models separately~\citep{wang2012joint}, which is inefficient~\citep{lin2002maximum}. 
Despite many multivariate models being presented in full generality, questions arrising from the high-dimensional context -- e.g. computational power, limits in numerical estimation, or sample size -- are never considered in analyses (to the best of our knowledge), and the number of longitudinal outcomes considered in numerical studies are often very low (see~\citet{hickey2016joint} for a complete review). For instance,~\citet{jaffa2014joint} only considers 3 longitudinal outcomes in the simulation study while mentioning a ``high-dimensional multivariate longitudinal data'' context.


\paragraph{General framework.}

The setting of this paper is such that we want to incorporate high-dimensional time-dependent (longitudinal) features measured with error in a survival model. Let us consider the usual survival analysis framework.
Following~\citet{AndBorGilKei-93}, let non-negative random variables $T^\star$ and $C$ stand for the times of the event of interest and censoring times respectively. The event of interest could be for instance survival time, re-hospitalization, relapse or disease progression.
We then denote $T$ the right-censored time and $\Delta$ the censoring indicator, defined as 
\begin{equation*}
T = T^\star \wedge C \quad \text{and} \quad \Delta = \ind{\{T^\star \leq C\}}
\end{equation*}
respectively, where $a \wedge b$ denotes the minimum between two numbers $a$ and $b$, and $\ind{\{\cdot\}}$ the indicator function taking the value $1$ if the condition in $\{\cdot\}$ is satisfied and $0$ otherwise.

Let $X$ denotes the $p$-dimensional vector of time-independent features (e.g. patients characteristics, therapeutic strategy, or omics features recorded at the begining of a study), and let  $Y(t) = \big(Y^1(t), \ldots, Y^L(t) \big)^\top \in \R^L$ denote the value of the $L$-dimensional longitudinal outcome at time point $t \geq 0$, with $L \in \N_+$.


\paragraph{Heterogeneity of the population.}

An assumption of heterogeneity within the patient population is frequently relevant in medical research where several differing profiles of subjects are expected~\citep{bussy2019c}. To take account of this, we introduce a latent variable $G \in \{0, \ldots, K-1\}$ modeling the $K \geq 1$ subgroups of different risk, which is a classical modeling assumption in JLCMs~\citep{lin2002latent, proust2014joint}. Let us denote
\begin{equation}
  \label{eq:pi}
  \pi_{\xi_k}(x) = \P[G=k|X=x]
\end{equation}
the latent class membership probability given time-independent features $x \in \R^p$, and consider a softmax link function given by
\[ \pi_{\xi_k}(x) = \dfrac{e^{x^\top\xi_k}}{\sum_{k=0}^{K-1}e^{x^\top\xi_k}} \]
where $\xi_k \in \R^p$ denotes a vector of coefficients that quantifies the impact of each time-independent features on the probability that a subject belongs to the $k$-th group, with $\xi_0 = \mathbf{0}_p$ for overparameterization purpose, where $\mathbf{0}_p$ stands for the vector of $\R^p$ having all coordinates equal to zero. The intercept term is here omitted without loss of generality. 
From now on, all computations are done conditionally on features $x$.

\paragraph{Main contribution.} 

In this paper, we propose a method called \textit{lights} (generaLized joInt hiGH-dimensional longiTudinal Survival) which is from both JLCMs and SREMs, since we also include features extracted from the longitudinal processes as potential risk factor in the survival model, which is a group-specific Cox model~\citep{Cox1972JRSS} with high-dimensional shared associations.
To allow flexibility in modeling the dependency between the longitudinal features and the event time, we use appropriate penalties : elastic net~\citep{zou2005regularization}
for feature selection in the latent class membership as well as within the survival model, and sparse group lasso~\citep{simon2013sparse} for the fixed effect of each marker trajectories, allowing flexible representations of time.
Inference is achieved using an efficient Quasi-Newton Expectation Maximization algorithm.
The posterior estimates of the latent class membership probabilities are then used as discriminative marker rule for risk prediction in the cross-validation procedure for selecting the best regularization hyper-parameters to adapt to the data complexity.
Hence, the method provides interpretations of the high-dimensional longitudinal features, thus offering a powerful tool for clinical decision making in patient monitoring.

\paragraph{Organization of the paper.}

A precise description of the model is given in Section~\ref{sec:Method}.
Section~\ref{sec:inference} focuses on a regularized version of the
model to exploit dimension reduction and prevent overfitting. Inference is
presented under this framework, as well as the developed algorithm.
Section~\ref{sec:Performance evaluation} introduces the C-index metric, as well as a novel evaluation strategy to assess diagnostic prediction performances while mimicking a real-time use of the model in clinical care, and finally the competing methods considered.
Section~\ref{sec:simulation study} presents the simulation procedure used to evaluate the performance of our method in a high-dimensional context and compares it with state-of-the-art ones. In Section~\ref{sec:application}, we apply our method to high-dimensional publicly available datasets. Finally, we discuss the obtained results in Section~\ref{sec:conclusion}.

\paragraph{Notations.}

Throughout the paper, for every $q > 0,$ we denote by $\norm{v}_q$ the usual $\ell_q$-quasi norm of a vector $v \in \R^m,$ namely
 $\norm{v}_q =(\sum_{k=1}^m|v_k|^q)^{1/q}$. We also denote $\norm{v}_0 = |\{k : v_k \neq 0\}|$, where $|A|$ stands for the cardinality of a finite set $A$.
 For $u, v \in \R^m$, we denote by $u \odot v$ the Hadamard product $u\odot v =(u_1v_1, \ldots, u_mv_m)^\top.$ 
For a squared matrix $M$, $\text{vech}(M)$ stacks columns of $M$ one under another in a single vector, starting each column at its diagonal element. We write $I_m$ for the identity matrix of $\R^{m \times m}$.
Finally, we write, for short, $\mathbf{1}_m$  (resp. $\mathbf{0}_m$) for the vector of $\R^m$ having all coordinates equal to one (resp. zero).


\section{Method}
\label{sec:Method}

In this section, we descibe the longitudinal and time-to-event submodels, as well as the required hypothesis in order to write a likelihood and draw inference for the lights model.

\subsection{Group-specific marker trajectories}

We suppose a group-specific marker trajectory and a generalized linear mixed model for each longitudinal marker given subgroup $G$, so that for the $l$-th outcome at time $t \geq 0$ one has
\begin{equation}
	\label{eq:link-function}
 	h_l\big(\E[Y^l(t)|b^l, G=k]\big) = m_k^l(t)
 \end{equation}
where $h_l$ denotes a known one-to-one link function, and $m_k^l$ the linear predictor such that
\[ m_k^l(t) = u^l(t)^\top\beta_k^l + v^l(t)^\top b^l \]
where $u^l(t) \in \R^{q_l}$ is a row vector of (possibly) time-varying features with corresponding unknown fixed effect parameters $\beta_k^l$, and $v^l(t) \in \R^{r_l}$ is a row vector of (possibly) time-varying features with $r_l \leq q_l$ and where the corresponding subject-and-longitudinal outcome specific random effects $b^l$ that does not depend on the group membership, which is not a strong modeling assumption.
\begin{assumption}
\label{assumption1}
We suppose that the random effects are independent of the group membership, and that the latter remain independent conditional on the observed data (namely $T$, $\Delta$ and $Y$).
\end{assumption}
A suitable distributional assumption for the random effects component is a zero-mean multivariate normal distribution~\citep{hickey2016joint}, that is
\[ b^l \sim \cN(0, D_{ll}) \]
with $D_{ll} \in \R^{r_l \times r_l}$ the unstructured variance-covariance matrix. 
To account for dependence between the different longitudinal outcome types, we let $\text{Cov}[b^l,b^{l'}] = D_{ll'}$ for $l \ne l'$ and we denote
\[ D = 
\begin{bmatrix}
  D_{11} & \cdots & D_{1L}\\
  \vdots &  \ddots & \vdots \\
  D_{1L}^\top & \cdots & D_{LL}
\end{bmatrix}
\]
the global variance-covariance matrix.
In the sequel of the paper, our aim is to associate the true and unobserved value $m_k^l(t)$ of the $l$-th longitudinal outcome at time $t$ with the event outcome $T^\star$.

\subsection{Group-specific risk of event}

To quantify the effect of the longitudinal outcomes on the risk for an event, we use a Cox~\citep{Cox1972JRSS} relative risk model of the form
\begin{equation}
	\label{eq:intensity-model}
	\lambda(t|\cM_k(t), G = k) = \lambda_0(t) \exp \Big\{ x^\top \gamma_{k,0} + \sum_{l=1}^L \sum_{a=1}^\cA {\gamma_{k,a}^l}^\top \varphi_a(t, \beta_k^l, b^l) \Big\},
\end{equation}
where $\lambda_0(\cdot)$ is an unspecified baseline hazard function and given that $G = k$, we denote 
\[\cM_k(t) = \{m_k^l(u), 0 \leq u < t\}\] 
the history of the true unobserved longitudinal process up to time $t$, $x$ is the $p$-vector of time-independant features (also used in~\eqref{eq:pi}, with no \textit{a priori} on the choice of features involved in the definition of $\pi_{\xi_k}$ nor $\lambda$, since independant regularizations are used during the inference, see~\eqref{eq:pen-log-lik}) with corresponding fixed effects $\gamma_{k,0} \in \R^p$, and for each $l$-th longitudinal outcome, we consider $\cA \in \N_+$ known functionals $\varphi_a$ defining a shared association with $\gamma_{k,a}^l \in \R^{\rho_a}$ the corresponding joint association parameters, and $\rho_a \in \N_+$ the dimension of the corresponding $\varphi_a$. 
This can be viewed as a generalization of SREMs~\citep{rizopoulos2010jm}. Let us finally denote
\[\gamma_k= (\gamma_{k,0}^\top, \gamma_{k,1}^1, \ldots, \gamma_{k,\cA}^1, \ldots, \gamma_{k,1}^L, \ldots, \gamma_{k,\cA}^L )^\top \in \R^{p+L\cA}. \]

\paragraph{Specification of the functionals $(\varphi_a)_{a\in \cA}$.}

The association structure between the longitudinal and the time-to-event submodels is key to the joint modeling framework. In spite of that, rationale for selecting shared associations has received little attention. We then propose to include some of the most common parameterization with no \textit{a priori}, set out in Table~\ref{table:shared_associations}, and let the model select the relevant ones through the regularization strategy described in Section~\ref{sec:penalized-obj}.
\begin{table}[htb]
\centering
\begin{tabular}{cccc}
\toprule
Description & $\varphi_a(t, \beta_k^l, b^l)$ & $\rho_a$ & Reference \\
\midrule
Linear predictor & $m_k^l(t)$ & 1 & \citet{chi2006joint} \\ [.15cm]
Random effects & $b^l$ & $r_l$ & \citet{hatfield2011joint} \\ 
Time-dependent slope & $\dfrac{\dd}{\dd t} m_k^l(t)$ & 1 & \citet{rizopoulos2011bayesian} \\ [.3cm]
Cumulative effect & $\int_0^t m_k^l(s) \dd s$ & 1 & \citet{andrinopoulou2017combined} \\ [.1cm]
 \bottomrule
\end{tabular}
\caption[]{Description of the shared associations we include in the group-specific risk of event submodel~\eqref{eq:intensity-model}.}
\label{table:shared_associations}
\end{table}
 
\subsection{Likelihood}

Consider an independent and identically distributed (i.i.d.) cohort of $n$ subjects
\[ \cD_n = \big\{ (x_1, y_1^1, \ldots, y_1^L, t_1, \delta_1), \ldots, (x_n, y_n^1, \ldots, y_n^L, t_n, \delta_n) \big\} \]
where for each subject $i=1, \ldots, n$, process $Y_i^l$ is measured $n_i^l$ times at $t_{i1}^l, \ldots, t_{in_i^l}^l$ (which can differ between subjects and outcomes) with $t_{ij}^l \leq t_{ij+1}^l$ for all $j=1, \ldots, n_i^l-1$ and such that
\[ y_i^l=(y_{i1}^l, \ldots, y_{in_i^l}^l)^\top \in \R^{n_i^l} \quad \text{ with } \quad y_{ij}^l=Y_i^l(t_{ij}^l) \]
for all $l=1, \ldots, L$. 
For the $i$-th subject, let us denote 
\[
\left\{
    \begin{array}{ll}
        y_i &= ({y_i^1}^\top \cdots {y_i^L}^\top)^\top \in \R^{n_i},\\
        b_i &= ({b_i^1}^\top \cdots {b_i^L}^\top)^\top \in \R^r,
    \end{array}
\right.
\]
with $n_i = \sum_{l=1}^L n_i^l$ and $r = \sum_{l=1}^L r_l$ the total number of longitudinal measurements (for subject $i$) and the total dimension of the random effects respectively, as well as the following design matrices
\[ U_i = 
\begin{bmatrix}
  U_{i1} & \cdots & 0\\
  \vdots &  \ddots & \vdots \\
  0 & \cdots & U_{iL}
\end{bmatrix} 
\in \R^{n_i \times q}
\qquad \text{and} \qquad
V_i = 
\begin{bmatrix}
  V_{i1} & \cdots & 0\\
  \vdots &  \ddots & \vdots \\
  0 & \cdots & V_{iL}
\end{bmatrix}
\in \R^{n_i \times r}
\]
with $q = \sum_{l=1}^L q_l$ and where for all $l=1, \ldots, L$, one writes
\[
\left\{
    \begin{array}{ll}
        U_{il} &= \big(u_i^l(t_{i1}^l)^\top \cdots u_i^l(t_{in_i^l}^l)^\top\big)^\top \in \R^{n_i^l \times q_l},\\
        V_{il} &= \big(v_i^l(t_{i1}^l)^\top \cdots v_i^l(t_{in_i^l}^l)^\top\big)^\top \in \R^{n_i^l \times r_l}.
    \end{array}
\right.
\]
From now on, all computations are done conditionally on the design matrices $(U_i)_{i=1, \dots, n}$ and $(V_i)_{i=1, \dots, n}$.
\begin{assumption}
Suppose that conditional on the random effects, each $Y_i^l$ is independent, and that the censoring times $T_i$ are independent of $b_i$.
\end{assumption}
This is a standard modeling assumption~\citep{tsiatis2004joint}.
For all $k = 0, \ldots, K-1$, we denote 
\[ \beta_k = ({\beta_k^1}^\top \cdots {\beta_k^L}^\top)^\top \in \R^q \] 
and 
\[M_{ik} = U_i\beta_k + V_ib_i \in \R^{n_i}. \]

Given~\eqref{eq:link-function}, each $y_i^l$ is assumed to be from a one-parameter exponential family with respect to a reference measure which is either the Lebesgue measure (e.g. in the Gaussian case) or the counting measure (e.g. in the logistic cases).
The conditional distribution of $y_i|b_i, G_i=k$ is then assumed to be from a distribution with a density of the form
\begin{equation}
	\label{eq:exp-family-density}
	f(y_i|b_i, G_i=k) = \exp \big\{(y_i \odot \Phi_i)^\top M_{ik} - \sum_{j=1}^{n_i} c_\phi(M_{ik, j}) + d_\phi(y_i) \big\},
\end{equation}
with 
\[ \Phi_i = (\phi_1^{-1} {\textbf{1}_{n_i^1}}^\top \cdots \phi_L^{-1} {\textbf{1}_{n_i^L}}^\top)^\top \in \R^{n_i} \] 
and $\phi = (\phi_1, \ldots, \phi_L)^\top \in \R^L$.
The density described in~\eqref{eq:exp-family-density} encompasses several distributions, see Table~\ref{table:glm}. The functions $c_\phi(\cdot)$ and $d_\phi(\cdot)$ are known as well as the dispersion parameters $\phi_l$, while parameters $\beta_k$ have to be estimated. Note that $d_\phi(\cdot)$ is related to the normalizing constant.
\begin{table}[htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccccc}
\toprule
 Model & Support & Use cases & $\phi_l$ & $h_l(\cdot)$ in~\eqref{eq:link-function} & $c_{\phi_l}(\cdot)$ \\
 \midrule
 Gaussian & $\R$ & Continuous response data & $\sigma_l^2$ & $z \mapsto z$ & $z \mapsto z^2 / 2\sigma_l^2$ \\ 
 Categorical & $\{0, 1\}^C$ & Outcome with $C$ modalities & 1 & $z \mapsto \log\Big(\dfrac{z}{1 - z} \Big)$ & $z \mapsto \log(1 + e^z)$ \\
 Poisson & $\N$ & Count of occurrences & 1 &  $z \mapsto \log(z)$ & $z \mapsto e^z$ \\
 \bottomrule
\end{tabular}}
\caption[]{Examples of standard distributions that fit in the considered setting.}
\label{table:glm}
\end{table}

Let us denote 
\[ \theta = \big(\xi_0^\top \cdots \xi_{K-1}^\top, \beta_0^\top \cdots \beta_{K-1}^\top, \phi^\top, \text{vech}(D), \lambda_0(t), \gamma_0^\top \cdots \gamma_{K-1}^\top\big)^\top \in \R^\vartheta \]
the collection of the $\vartheta \in \N_+$ unknown parameters to estimate.
To write the log-likelihood $\ell_n(\theta)$ (rescaled by $n^{-1}$)  for samples in $\cD_n$, corresponding to the joint distribution of the time-to-event and longitudinal outcomes, let us assume that both the random effects vector $b_i$ and the group membership account for the association between the longitudinal and event outcomes, that is
\begin{equation}
	\label{eq:ind-hyp}
	f(t_i, \delta_i, y_i | b_i, G_i = k ; \theta) = f(t_i, \delta_i| b_i, G_i = k ; \theta) f(y_i | b_i, G_i = k ; \theta)
\end{equation}
for all $i=1, \ldots, n$. Hypothesis~\eqref{eq:ind-hyp} is a generalization of classical hypothesis used in SREMs and JLCMs~\citep{hickey2016joint}.
Then, one has
\begin{align}
	\label{eq:log-lik}
	\ell_n(\theta) &= \ell_n(\theta ; \cD_n) \nonumber \\
	&= n^{-1} \sum_{i=1}^n \log \int_{\R^r} \sum_{k=0}^{K-1} \pi_{\xi_k}(x_i) f(t_i, \delta_i| b_i, G_i = k ; \theta) f(y_i | b_i, G_i = k ; \theta) f(b_i ; \theta) \dd b_i,
\end{align}
where 
\[f(t_i, \delta_i| b_i, G_i = k ; \theta) = \big[\lambda(t_i|\cM_k(t_i), G_i = k)\big]^{\delta_i} \exp \Big\{-\int_0^{t_i} \lambda(s|\cM_k(s), G_i = k) \dd s \Big\} \]
and
\begin{equation}
  \label{eq:b_density}
  f(b_i ; \theta) = (2 \pi)^{-\dfrac r2} |D|^{-\dfrac12} \exp \Big\{ -\dfrac12 b_i^\top D^{-1}b_i \Big\}.
\end{equation}

\paragraph{Specification of the design matrices.}
 
In many practical applications, subjects show highly nonlinear longitudinal trajectories. In~\eqref{eq:log-lik}, the complete longitudinal history is required for the computation of the survival function. Hence, in order to produce a good estimate of $\cM(t)$, we consider a flexible representations for $u^l(t)$ using a high-dimensional vector of polynomial functions of time $t$, namely 
\[u^l(t) = (1, t, t^2, \ldots, t^\alpha)^\top.\]
% TODO: en pratique essayer d'ajouter X dans u^l(t) pr voir si ça améliore les perf ou non. Si oui l'ajouter au modèle.
The idea here is to allow a wide range of orders for the polynomial so that a suitable one can be automatically chosen for each trajectory -- depending on its inherent complexity -- thanks to the regularization (see~\eqref{eq:pen-log-lik}).
We then let \[v^l(t) = (1, t)^\top\] so that each trajectory of each subject gets an affine random effect. Hence with this choice in practice, one has $q_l=\alpha + 1$ and $r_l=2$ for for all $l=1, \ldots, L$.

\section{Inference}
\label{sec:inference}

In this section, we describe the procedure for estimating the parameters of the lights model. We begin by presenting the considered penalized objective, and then focus on the algorithm we use for inference.

\subsection{Penalized objective}
\label{sec:penalized-obj}

In order to avoid overfitting and improve the prediction power of our method, we propose to minimize the penalized objective
\begin{equation}
  \label{eq:pen-log-lik}
	\ell_n^\text{pen}(\theta) = - \ell_n(\theta) + \sum_{k=0}^{K-1} \zeta_{1,k} \norm{\xi_k}_{\text{en}, \eta} + \zeta_{2,k} \norm{\gamma_k}_{\text{en}, \eta} + \zeta_{3,k} \norm{\beta_k}_{\text{sg} l_1, \tilde{\eta}}
\end{equation}
where for all $k=0, \ldots, K-1$, we add an elastic net regularization~\citep{zou2005regularization} of the vectors $\xi_k$ and $\gamma_k$ and a sparse group lasso regularization~\citep{simon2013sparse} of the vector $\beta_k$, for tuning hyper-parameters $(\zeta_{1,k}, \zeta_{2,k}, \zeta_{3,k})^\top \in \R_+^3$. Here, $(\eta, \tilde{\eta}) \in [0, 1]^2$ are fixed and we denote
\[ \norm{v}_{\text{en}, \eta} = (1-\eta)\norm{v}_1 + \dfrac\eta2 \norm{v}_2^2 \]
for any vector $v$, that is a linear combination of the lasso ($\ell_1$) and ridge (squared $\ell_2$) penalties, and
\[ \norm{\beta_k}_{\text{sg} l_1, \tilde{\eta}} = (1-\tilde{\eta})\sum_{l=1}^L\norm{\beta_k^l}_2 + \tilde{\eta} \norm{\beta_k}_1 \] for the sparse group lasso penalty, for which we do not have to account for group sizes since they all have the same one. Hence, the resulting optimization problem is written
\begin{equation}
  \label{eq:optim-pb}
   \hat \theta \in \argmin_{\theta \in \R^\vartheta} \ell_n^\text{pen}(\theta).
 \end{equation}

One advantage of the considered regularization method is its ability to perform feature selection (the lasso part) and pinpoint the most important features relatively to the prediction objective : the support of $\xi_k$ (respectively $\gamma_k$) thus informs on the features involved in the $k$-th group membership (in the $k$-th group risk of event respectively).
The ridge part allows to handle potential correlation between time-independent features or between longitudinal outcomes.
Note that in practice, the intercept is not regularized.
On the other hand, the sparse group lasso of $\beta_k$ allows to consider a flexible representation of time for the design matrices $(U_i)_{i=1, \dots, n}$ and lets the model automatically fit each trajectory with the right complexity.

\subsection{A Quasi-Newton Monte Carlo EM}
\label{sec:QNMCEM}

In order to derive an algorithm for this objective, we introduce a so-called QNMCEM algorithm, being a combination
between an EM algorithm~\citep{dempster1977maximum} with Monte Carlo approximations~\citep{levine2001implementations}, and multiple L-BFGS-B algorithms~\citep{zhu1997algorithm}. EM algorithm has already been used for multivariate data joint modeling (see~\citet{lin2002maximum} for instance), but here we face different original problems: for each subject $i$, the latent variables are the pairs $(G_i, b_i)$ (not only the random effects); and then, we want to minimize the penalized objective $\ell_n^\text{pen}$ (not ``only'' the negative log-likelihood).

We first need to compute the negative completed log-likelihood (here scaled by $n^{-1}$), namely the negative joint distribution of $\cD_n$, $\textbf{\textit{b}} = (b_1, \ldots, b_n)$ and $\textbf{\textit{G}} = (G_1, \ldots, G_n)$.
It can be written
\begin{align*}
  \ell_n^\text{comp}(\theta) &= \ell_n^\text{comp}(\theta ; \cD_n, \textbf{\textit{b}}, \textbf{\textit{G}}) \\ 
  &= - n^{-1} \sum_{i=1}^n -\dfrac12 (r \log 2\pi + \log|D| + b_i^\top D^{-1}b_i) + \sum_{k=0}^{K-1} \ind{\{G_i=k\}} \Big[ \log \pi_{\xi_k}(x_i) \\ 
  & \quad + \delta_i \Big(\log \lambda_0(t_i) + x_i^\top \gamma_{k,0} + \sum_{l=1}^L \sum_{a=1}^\cA {\gamma_{k,a}^l}^\top \varphi_a(t_i, \beta_k^l, b_i^l) \Big) \\
  & \quad -\int_0^{t_i} \lambda_0(s) \exp \Big\{ x_i^\top \gamma_{k,0} + \sum_{l=1}^L \sum_{a=1}^\cA {\gamma_{k,a}^l}^\top \varphi_a(s, \beta_k^l, b_i^l) \Big\} \dd s \\
  & \quad + (y_i \odot \Phi_i)^\top M_{ik} - \sum_{j=1}^{n_i} c_\phi(M_{ik, j}) + d_\phi(y_i) \Big].
\end{align*}
Suppose that we are at step $w + 1$ of the algorithm, with current iterate denoted $\theta^{(w)}$. 

\paragraph*{Monte Carlo E-step.}

We need to compute the expected negative log-likelihood of the complete data conditional on the observed data and the current estimate of the parameters given by 
\begin{equation*}
  \cQ_n(\theta, \theta^{(w)}) = \E_{\theta^{(w)}}[\ell_n^\text{comp}(\theta) | \cD_n].
\end{equation*}
Given Assumption~\ref{assumption1}, the previous expression requires to compute expectations of the form
\[ \E_{\theta^{(w)}}[ g(b_i, G_i) | t_i, \delta_i, y_i] = \sum_{k=0}^{K-1} \pi_{ik}^{\theta^{(w)}} \int_{\R^r} g(b_i, G_i) f(b_i | t_i, \delta_i, y_i ; \theta^{(w)}) \dd b_i \]
for different functions $g$, where we denote 
\begin{equation}
  \label{eq:pi_ik-def}
  \pi_{ik}^{\theta^{(w)}} = \P_{\theta^{(w)}}[G_i = k | t_i, \delta_i, y_i] 
\end{equation}
the posterior probability of the latent class membership using parameters $\theta^{(w)}$. 
Then, either $g(b_i, G_i) = \tilde g(bi)$ (for instance $\tilde g: b_i \mapsto b_i^\top D^{-1}b_i$), or $g(b_i, G_i) = g_k(Gi)$ with $g_k : G_i \mapsto \ind{\{G_i=k\}}$.

In the case $g(b_i, G_i) = \tilde g(bi)$, one has
\begin{equation*}
  \E_{\theta^{(w)}}[ \tilde g(b_i) | t_i, \delta_i, y_i] = \int_{\R^r} \tilde g(b_i) f(b_i | t_i, \delta_i, y_i ; \theta^{(w)}) \dd b_i = \dfrac{ \sum_{k=0}^{K-1}  \Lambda_{ik,\tilde g}^{\theta^{(w)}}}{\sum_{k=0}^{K-1} \Lambda_{ik,1}^{\theta^{(w)}}}
\end{equation*}
with
\begin{equation}
  \label{eq:Lambda_ik-def}
  \Lambda_{ik,\tilde g}^{\theta^{(w)}} = \pi_{\xi_k^{(w)}}(x_i) \int_{\R^r} \tilde g(b_i) f(t_i, \delta_i, y_i | b_i, G_i = k ; \theta^{(w)}) f(b_i ; \theta^{(w)}) \dd b_i,
\end{equation}
and $\Lambda_{ik,1}^{\theta^{(w)}}$ obtained from $\Lambda_{ik,\tilde g}^{\theta^{(w)}}$ taking $\tilde g: b_i \mapsto 1$.
The integral in~\eqref{eq:Lambda_ik-def} is intractable analytically, and some form of approximation must be used in practice. Standard numerical integration techniques (such as Gaussian quadrature) are not well suited in our high-dimensional context. We then propose to use Monte Carlo approximation, which has already been applied for generalized linear mixed models~\citep{booth1999maximizing}, with antithetic normal variates method~\citep{hammersley1956new} to reduce variance of the simulation result. Algorithm~\ref{alg:construct-set-S} describes how to construct the set $S^{(w)}$ used for the approximation.

\vspace{.5cm}

\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Construction of the samples in $S^{(w)}$}
\label{alg:construct-set-S}
\KwInput{$S^{(w)} = \varnothing$}
\KwOutput{$S^{(w)}$ filled with $2N$ samples}
Compute $C^{(w)} \in \R^{r \times r}$ such that $C^{(w)}{C^{(w)}}^\top = D^{(w)}$ \tcp{Cholesky decomposition}
\For{$\breve{n} = 1,\ldots,N$}    
{ 
  Sample $\Omega_{\breve{n}} \sim \cN(0, I_r)$ \\
  Compute $\breve{b}_{\breve{n}} = C^{(w)} \Omega_{\breve{n}} \in \R^r$ \\
  Update $S^{(w)} \leftarrow S^{(w)} \cup \{\breve{b}_{\breve{n}}, -\breve{b}_{\breve{n}} \}$
}
\textbf{Return:} {$S^{(w)}$}
\end{algorithm}

\vspace{.5cm}

\noindent The approximation of~\eqref{eq:Lambda_ik-def} is finally obtained by
\begin{equation}
  \label{eq:hat_Lambda_ik}
  \hat \Lambda_{ik,\tilde g}^{\theta^{(w)}} = \pi_{\xi_k^{(w)}}(x_i) \dfrac{1}{2N} \sum_{\breve{b} \in S^{(w)}} \tilde g(\breve{b}) f(t_i, \delta_i, y_i | \breve{b}, G_i = k ; \theta^{(w)}).
\end{equation}

It is common in Monte Carlo EM to increase $N$ with the iterations $w$~\citep{wei1990monte} since it is computationally inefficient to use a large $N$ in the early steps of the algorithm, when $\theta^{(w)}$ is far from $\hat \theta$. Multiple techniques have been proposed (see for instance~\citet{law2002joint} where a subjectively rule is followed, or~\citet{booth1999maximizing} that uses a rule based on confidence intervals for $\theta^{(w)}$ which requires additional variance estimation). We opt for a simple automated approach using relative differences of the objective function defined in~\eqref{eq:pen-log-lik} (see Algorithm~\ref{alg:QNMCEM}).

Now in the case $g = g_k$, one has $\E_{\theta^{(w)}}[g_k(Gi) | t_i, \delta_i, y_i] = \pi_{ik}^{\theta^{(w)}}$ and we compute its approximation given
\begin{equation}
\label{eq:hat_pi_ik-def}
  \hat \pi_{ik}^{\theta^{(w)}} = \dfrac{\hat \Lambda_{ik,1}^{\theta^{(w)}}}{\sum_{k=0}^{K-1} \hat \Lambda_{ik,1}^{\theta^{(w)}}}.
\end{equation}

\paragraph*{Quasi-Newton M-step.}
Here, we need to compute \[\theta^{(w+1)} \in \argmin_{\theta \in \R^\vartheta} \cQ_n(\theta, \theta^{(w)}) .\]
First, for the update of $\lambda_0^{(w)}$, we treat the jump size at the observed event times as parameters to be estimated~\citep{klein1992semiparametric}. The closed-form update is then given by
\begin{equation}
  \label{eq:update_lambda_0}
  \lambda_0^{(w+1)}(t)= \dfrac{\sum_{i=1}^n \delta_i \ind{\{t=t_i\}}}{\sum_{i=1}^n \sum_{k=0}^{K-1} \hat \Lambda_{ik,\tilde g}^{\theta^{(w)}} \ind{\{t_i \geq t\}} } ,
\end{equation}
with $\tilde g: b_i \mapsto \exp \Big\{ x_i^\top \gamma_{k,0} + \sum_{l=1}^L \sum_{a=1}^\cA {\gamma_{k,a}^l}^\top \varphi_a(t_i, \beta_k^l, b_i^l) \Big\}$, which is a Breslow like estimator~\citep{breslow1972contribution} adapted to our model.
Then, the update of $D^{(w)}$ is naturally given in closed-form by
\begin{equation}
  \label{eq:update_D}
  D^{(w+1)}= n^{-1} \dfrac{1}{2N} \sum_{\breve{b} \in S^{(w)}} \breve{b} \breve{b}^\top f(t_i, \delta_i, y_i | \breve{b}, G_i = k ; \theta^{(w)}).
\end{equation}
Concerning the update of $\phi^{(w)}$ now, and regarding Table~\ref{table:glm}, let us focus on the Gaussian case being particularly useful in practice.


\paragraph*{Initialization.}
Because our algorithm gives a local minimum, it is clever to choose an initial value $\theta^{(0)}$ relatively close to the final solution $\hat \theta$.


\paragraph*{The QNMCEM algorithm.}
Algorithm~\ref{alg:QNMCEM} describes the main steps of the resulting QNMCEM algorithm to solve the optimisation problem~\eqref{eq:optim-pb}.

\vspace{.5cm}

\begin{algorithm}[H]
\DontPrintSemicolon
\caption{QNMCEM algorithm for inference of the lights model}
\label{alg:QNMCEM}
\KwInput{Training data $\cD_n$; initialize $N=50L$; tuning hyper-parameters $(\zeta_{1,k}, \zeta_{2,k}, \zeta_{3,k})_{k=0,\ldots,K-1}$}
\KwOutput{Last parameters $\hat \theta \in \R^\vartheta$}
\tcc{Initialization}
Compute the starting parameters $\theta^{(0)} \in \R^\vartheta$ following ?? \\
\For{$w = 1,\ldots,$ until convergence}    
{ 
  \tcc{Monte Carlo E-step}
  Compute $\hat \Lambda_{ik,\tilde g}^{\theta^{(w)}}$ using~\eqref{eq:hat_Lambda_ik}\\
  Compute $\hat \pi_{ik}^{\theta^{(w)}}$ using~\eqref{eq:hat_pi_ik-def}\\
  \If{Condition}
  {
      $N \leftarrow N + $
  }
  \tcc{Quasi-Newton M-step}
  Update $\lambda_0^{(w+1)}$ using~\eqref{eq:update_lambda_0}\\
  Update $D^{(w+1)}$ using~\eqref{eq:update_D}\\
  
}
\textbf{Return:} {$\hat \theta$}
\end{algorithm}


\subsection{Practical details}
\label{sec:practical details}

Let us give some numerical details about the choices made for using the lights model in practice.

\paragraph{Cross-validation procedure.}

When validating predictions or comparing performances of competing models in a survival context, one can be interested either in the discriminative power of the predictive rule and use concordance measures such as the C-index~\citep{heagerty2005survival} (defined in Section~\ref{sec:Metrics}), or in the predictive accuracy of the rule~\citep{schemper2000predictive}.
Developments in the field of joint modeling have primarily focused on modeling and estimation, and most studies do not consider goodness of fit nor generalisation power in a prognostic prediction perspective~\citep{hickey2016joint}. However, practitioners will naturally require predictive prognostic tools to evaluate model fits and compare models.

The lights model has the ability to produce prognostic predictions that can be dynamically updated according to the observed trajectory of the features. 
Once the model is trained (so that one obtains $\hat\theta$ from~\eqref{eq:optim-pb} using our QNMCEM algorithm introduced in Section~\ref{sec:QNMCEM}), posterior risk classification can be made through $\hat \pi_{ik}^{\hat\theta}$ (see~\eqref{eq:hat_pi_ik-def}).
We propose to use this probabilities as discriminative marker rule for risk prediction and use it in the cross-validation procedure for selecting the best regularization hyper-parameters. Indeed, it makes sense from a practical point of view, since one wants to obtain prognostic predictions as well as interpretation of the high-dimensional longitudinal features.

\paragraph{The case $K=2$.}

In many practical applications -- including those addressed in Section~\ref{sec:application} -- we are interested in identifying one subgroup of the population with a high risk of adverse event compared to the others. Then, in the following, we consider $G \in \{ 0, 1\}$ where $G=1$ means high-risk of early adverse event and $G=0$ means low risk.
To simplify notations, let us set $\xi = \xi_1$, $\pi_\xi(x)$ the conditional probability that a patient belongs to the group with high risk of adverse event given its time-independent features $x$, and $\zeta_1 = \zeta_{1,1}$.
In this context, which is the one we consider in practice in Sections~\ref{sec:simulation study} and~\ref{sec:application}, we suppose that the ``right'' penalty strength for $\xi_k$ and $\gamma_k$ does not depend on $k$, so that we denote $\zeta_2 = \zeta_{2,0} = \zeta_{2,1}$ and $\zeta_3 = \zeta_{3,0} = \zeta_{3,1}$. 

The hyper-parameters $(\zeta_1, \zeta_2, \zeta_3)^\top \in \R_+^3$ are then tuned during a 10-fold randomized search cross-validation procedure~\citep{bergstra2012random} using marker $\hat \pi_{ik}^{\hat\theta}$ (see~\eqref{eq:hat_pi_ik-def}) and the C-index score defined in Section~\ref{sec:Metrics}.
Random search are indeed more efficient in terms of computing times than classical grid search for hyper-parameter optimization and finds better models by effectively searching a larger configuration space, which is appropriate to our high-dimensional problem.
% TODO: Figure de visualisation de la CV de (\zeta_1, \zeta_2, \zeta_3) en 3D


\section{Performance evaluation}
\label{sec:Performance evaluation}

In this section, we first introduce the C-index metric used to assess performances and briefly introduce the models we consider for performance comparisons. Then, we provide details regarding the simulation study and data generation. The chosen metrics for evaluating performances are then presented, followed by the results.

\subsection{The C-index metric}
\label{sec:Metrics}

We detail in this section the metric considered to evaluate risk prediction performances. Let us denote by $M$ the marker under study (e.g. the posterior group membership probabilities $\hat \pi_{ik}^{\hat\theta}$ (see~\eqref{eq:hat_pi_ik-def}) for the lights model), and assume that it is measured once at $t = 0$.
A common concordance measure that does not depend on time is the C-index~\citep{harrell1996tutorial} defined by
\begin{equation*}
  \cC =\P[M_i > M_j | T^\star_i < T^\star_j],
\end{equation*}
with $i \neq j$ two independent subjects (which does not depend on $i, j$ under the i.i.d. sample hypothesis). 
In our case,  $T^\star$ is subject to right censoring, so one would typically consider the modified $\cC_\tau$ defined by
\begin{equation*}
  \cC_\tau =\P[M_i > M_j | T_i < T_j , T_i < \tau],
\end{equation*}
with $\tau$ corresponding to the fixed and prespecified follow-up period duration \citep{heagerty2005survival}. A Kaplan-Meier estimator for the censoring distribution leads to a nonparametric and consistent estimator of $\cC_\tau$ \citep{uno2011c}, already implemented in the \texttt{Python} package \texttt{lifelines}.
Hence in the following, we consider the C-index metric to assess performances.


\subsection{Evaluation strategy}
\label{sec:evaluation strategy}


\subsection{Competing models}
\label{sec:competing models}

In this section, we briefly introduce the models we consider for performance comparisons in the simulation study as well as in the applications on real datasets in Section~\ref{sec:application}.

\paragraph*{Penalized Cox model with time-independant features.}

The first model we consider as a baseline is the well known Cox PH model with time-independant features.
In this model introduced in \citet{Cox1972JRSS}, a parameter vector $\beta$ is estimated by minimizing the partial log-likelihood given by
\begin{equation*}
  \ell_n^{\text{cox}}(\beta) = n^{-1} \sum_{i=1}^n \delta_i \big( x_i^\top \beta - \log \sum_{i' : t_{i'} \geq t_i} \text{exp}(x_{i'}^\top \beta) \big).
\end{equation*}
We use respectively the \texttt{R} packages \texttt{survival} and \texttt{glmnet} \citep{simon2011regularization} for the partial log-likelihood and the minimization of the following quantity
\begin{equation*}
  - \ell_n^{\text{cox}}(\beta) + \xi \big( (1-\eta)\norm{\beta}_1 + \frac{\eta}{2} \norm{\beta}_2^2 \big),
\end{equation*}
where $\xi$ is chosen by the a 10-fold cross-validation procedure, for a given $\eta \in [0, 1]$. Ties are handled via the Breslow approximation of the partial likelihood \citep{breslow1972contribution}. We also choose to include basic time-independant features extracted from longitudinal processes, namely the average value for each time-dependant feature.

\paragraph*{The time-dependent Cox model with \texttt{survival} package.}

A classical extension of the Cox model supposes that features depend on time~\citep{sueyoshi1992semiparametric} (and then loosing the risk proportionality property), that is with our notations
\[\lambda_i(t) = \lambda_0(t) \exp\big(y_i(t)^\top \beta \big). \]
Using the \texttt{survival} \texttt{R} package~\citep{zhang2018time}, blabla description...


\paragraph*{Multivariate joint modeling with \texttt{joineRML} package.}
In this \texttt{R} package~\citep{hickey2018joinerml}, blabla description...

\paragraph*{The \texttt{JM} package.}
In this \texttt{R} package~\citep{rizopoulos2010jm}, blabla description...

\paragraph*{The \texttt{JMbayes} package.}
In this \texttt{R} package~\citep{rizopoulos2014r}, blabla description...

% TODO: voir slide 132 \url{http://www.drizopoulos.com/courses/Int/JMwithR_CEN-ISBS_2017.pdf})

\paragraph*{The \texttt{joineR} package.}
In this \texttt{R} package~\citep{philipson2018package}, blabla description...

\section{High-dimensional simulation study}
\label{sec:simulation study}

\subsection{Simulation design}
\label{simulation design}

In order to assess the proposed method, we perform an extensive Monte Carlo simulation study. \\
\\
simulation of time-varying features in a Cox model: see \citet{therneau2017using}

\subsection{Results of simulation}
\label{sec:simulation results}
Let us present now the simulation results.


\section{Applications}
\label{sec:application}

In this section, we apply our lights method on two publicly available datasets and compare its performance with state-of-the-art methods.

\subsection{The PBCseq dataset}

This dataset is a follow-up to the original dataset~\citep{fleming2011counting, murtaugh1994primary}from the Mayo Clinic trial in primary biliary cirrhosis (PBC) of the liver conducted between 1974 and 1984. 
A total of 424 PBC patients, referred to Mayo Clinic during that ten-year interval, met eligibility criteria for the randomized placebo controlled trial of the drug D-penicillamine. The first 312 cases in the data set participated in the randomized trial and contain largely complete data. The additional 112 cases did not participate in the clinical trial, but consented to have basic measurements recorded and to be followed for survival. Six of those cases were lost to follow-up shortly after diagnosis, so the data here are on an additional 106 cases as well as the 312 randomized participants. 
The dataset contains only baseline measurements of the laboratory paramters and contains multiple laboratory results, but only on the first 312 patients. 


\subsection{The MIMIC III dataset}

The MIMIC III (Medical Information Mart for Intensive Care III) database is a large, freely-available hospital dataset containing de-identified data from over 40,000 patients. This data comes from patients who were admitted in critical care units to Beth Israel Deaconess Medical Center in Boston, Massachusetts from 2001 to 2012~\citep{johnson2016mimic}. The dataset was populated with data that had been acquired during routine hospital care, so there was no associated burden on caregivers and no interference with their workflow.

% \url{https://www.nature.com/articles/sdata201635/}\\
% \url{https://ieeexplore.ieee.org/abstract/document/1166854}\\
% \url{https://www.sciencedirect.com/science/article/abs/pii/S2213260014702395}\\
% \url{https://www.sciencedirect.com/science/article/pii/S2352556818302169}\\
% }


\section{Conclusion}
\label{sec:conclusion}

In this paper, a generalized joint model for high-dimensional multivariate longitudinal data and censored durations (lights) has been introduced, and a new efficient estimation
algorithm (QNMCEM) has been derived, that considers a penalization of the likelihood in order to perform covariate
selection and to prevent overfitting.


\section*{Software}
All the methodology discussed in this paper is implemented in \texttt{Python}. The code is available from \href{https://github.com/SimonBussy/lights}%
{https://github.com/SimonBussy/lights} in the form of annotated programs, together with a notebook tutorial.

\section*{Acknowledgements}
\textit{Conflict of Interest}: None declared.

% \appendix

% \begin{center}
% \LARGE \textbf{Appendices}
% \end{center}

% \section{Proofs}
% \label{sec:proofs}


\bibliography{biblio}
\bibliographystyle{plainnat}{}
\end{document}
